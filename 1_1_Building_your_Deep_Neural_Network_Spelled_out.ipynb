{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlhqnWDwuBwkh3ohSqNaUV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andreidhoang/deep_learning_from_first_principles/blob/main/1_1_Building_your_Deep_Neural_Network_Spelled_out.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Big Picture: Building Blocks of a Neural Network"
      ],
      "metadata": {
        "id": "mzFKDey7hk_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Operations:**\n",
        "- NN perform computations. These computations can be broken down into \"operations\"\n",
        "- Matrix multiplication (to calculate weighted sums), adding biases, applying activation functions.\n",
        "- the `Operation` class is the base class for all these individual computations.\n",
        "\n",
        "**2. Layers:**\n",
        "- A layer is a collection of neurons. Each neuron applies a series of operations to its inputs.\n",
        "- A neural network is made up of layers stacked together.\n",
        "- The `Layer` class in our code organizes a set of Operations.\n",
        "\n",
        "**3. Activation Functions:**\n",
        "\n",
        "- These introduce non-linearity to the network, allowing it to learn complex relationships in data.\n",
        "- Examples: Sigmoid, ReLU (Rectified Linear Unit), Tanh.\n",
        "- In our code, activation functions are a type of Operation.\n",
        "\n",
        "**4. Loss Function:**\n",
        "\n",
        "- This measures how well the network is performing (how far off its predictions are from the actual values).\n",
        "- The goal of training is to minimize this loss.\n",
        "- The Loss class and its subclasses (like MeanSquaredError) calculate this.\n",
        "\n",
        "**5. Neural Network:**\n",
        "\n",
        "- The overall architecture that connects layers together.\n",
        "- It defines the flow of data through the network (forward pass) and the flow of gradients for learning (backward pass).\n",
        "- The NeuralNetwork class orchestrates the layers and the loss function.\n",
        "\n",
        "**6. Optimizer:**\n",
        "\n",
        "- This algorithm updates the network's parameters (weights and biases) based on the calculated gradients to minimize the loss function.\n",
        "- Example: Stochastic Gradient Descent (SGD).\n",
        "The Optimizer class (and SGD) handles this update process.\n",
        "\n",
        "**7. Trainer:**\n",
        "\n",
        "- This class manages the training loop, feeding data to the network, calculating the loss, updating parameters, and evaluating performance.\n",
        "- The Trainer class encapsulates the training logic.\n"
      ],
      "metadata": {
        "id": "d4KHe5L5hn58"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "REiwC9FnjRVU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB7ygadjheNG",
        "outputId": "ac0ad904-1bd0-4b49-812f-848ffd180d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original array:\n",
            "[1 2 3 4]\n",
            "Shape: (4,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import ndarray\n",
        "from typing import List, Tuple  # Import Tuple\n",
        "\n",
        "def assert_same_shape(array: ndarray, array_grad: ndarray) -> None:\n",
        "  \"\"\"\n",
        "  Asserts that two ndarrays have the same shape.\n",
        "  \"\"\"\n",
        "  assert array.shape == array_grad.shape, \\\n",
        "    f\"Two ndarrays should have the same shape; instead, first ndarray's shape is {array_grad.shape} and second ndarray's shape is {array.shape}.\"\n",
        "\n",
        "def to_2d_np(a: ndarray, type: str = \"col\") -> ndarray:\n",
        "    \"\"\"\n",
        "    Reshapes a 1D ndarray to a 2D ndarray.\n",
        "\n",
        "    Args:\n",
        "      a: The 1D ndarray to reshape.\n",
        "      type:  \"col\" to make it a column vector (shape: (n, 1)),\n",
        "              \"row\" to make it a row vector (shape: (1, n)).\n",
        "\n",
        "    Returns:\n",
        "        The reshaped 2D ndarray.\n",
        "    \"\"\"\n",
        "    assert a.ndim == 1, \"Input ndarray must be 1-dimensional.\"\n",
        "    if type == \"col\":\n",
        "        return a.reshape(-1, 1)  # -1 infers the number of rows\n",
        "    elif type == \"row\":\n",
        "        return a.reshape(1, -1)  # -1 infers the number of columns\n",
        "    else:\n",
        "        raise ValueError(\"Type must be 'col' or 'row'.\")\n",
        "\n",
        "# Let's say we have a 1D array representing some target values:\n",
        "my_array_1d = np.array([1, 2, 3, 4])\n",
        "print(\"Original array:\")\n",
        "print(my_array_1d)\n",
        "print(\"Shape:\", my_array_1d.shape)  # Output: (4,)  (A 1D array with 4 elements)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's use to_2d_np to reshape it into a column vector:\n",
        "my_array_2d_col = to_2d_np(my_array_1d, type=\"col\")\n",
        "print(\"\\nReshaped to column vector:\")\n",
        "print(my_array_2d_col)\n",
        "print(\"Shape:\", my_array_2d_col.shape)  # Output: (4, 1) (A 2D array with 4 rows and 1 column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xw5Ro3VhjdDc",
        "outputId": "3f2a5c2e-eebd-4031-d7f4-18493f461944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reshaped to column vector:\n",
            "[[1]\n",
            " [2]\n",
            " [3]\n",
            " [4]]\n",
            "Shape: (4, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape to a row vector:\n",
        "my_array_2d_row = to_2d_np(my_array_1d, type=\"row\")\n",
        "print(\"\\nReshaped to row vector:\")\n",
        "print(my_array_2d_row)\n",
        "print(\"Shape:\", my_array_2d_row.shape)  # Output: (1, 4) (A 2D array with 1 row and 4 columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrJN0x1TkoLh",
        "outputId": "ca4a5a80-2caa-4b50-96e7-1a2fffbaeb37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reshaped to row vector:\n",
            "[[1 2 3 4]]\n",
            "Shape: (1, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operations"
      ],
      "metadata": {
        "id": "z3ZjhvxTlMPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Operation:\n",
        "  \"\"\"\n",
        "  base class for all operations in our Neural Network\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.input_ = None\n",
        "\n",
        "  def forward(self, input_: ndarray) -> ndarray:\n",
        "    \"\"\"\n",
        "    Performs the forward pass of the operation.\n",
        "\n",
        "    Args:\n",
        "      input_: The input ndarray.\n",
        "\n",
        "    Returns:\n",
        "      The output ndarray.\n",
        "    \"\"\"\n",
        "\n",
        "    self.input_ = input_\n",
        "    return self._output()\n",
        "\n",
        "  def backward(self, output_grad: ndarray) -> ndarray:\n",
        "    \"\"\"\n",
        "    Performs the backward pass of the operation.\n",
        "\n",
        "    Args:\n",
        "      Output_grad: The gradient of the loss with respect to the output of this operation.\n",
        "\n",
        "    Returns:\n",
        "      The gradient of the loss with respect to the input of this operation.\n",
        "    \"\"\"\n",
        "    assert_same_shape(self.input_, output_grad)\n",
        "    return self._input_grad(output_grad)\n",
        "\n",
        "  def _output(self) -> ndarray:\n",
        "        \"\"\"\n",
        "        Calculates the output of the operation.\n",
        "        This method needs to be implemented by the subclasses.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "  def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Calculates the gradient with respect to the input.\n",
        "        This method needs to be implemented by the subclasses.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "  @property\n",
        "  def output(self) -> ndarray:\n",
        "        \"\"\"\n",
        "        Returns the output of the operation (calculated during the forward pass).\n",
        "        \"\"\"\n",
        "        return self._output()  # We can directly call _output here for simplicity for now"
      ],
      "metadata": {
        "id": "hrX5FUd3k4DA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ParamOperation Class**\n",
        "- It provides a blueprint for operations that learn. In neural networks, weights and biases are the parameters that the network adjusts to improve its performance.\n",
        "- It cleanly separates the calculation of gradients for inputs (_input_grad) and parameters (_param_grad), which are used for different purposes during backpropagation."
      ],
      "metadata": {
        "id": "Zf3kTKnfptlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ParamOperation(Operation):\n",
        "    \"\"\"\n",
        "    An Operation with parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, param: ndarray):\n",
        "        \"\"\"\n",
        "        Initialize with a parameter.\n",
        "\n",
        "        Args:\n",
        "            param: The parameter (e.g., weights or biases).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.param = param\n",
        "\n",
        "\n",
        "    def backward(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Performs the backward pass.\n",
        "        Calculates the gradient with respect to the input (_input_grad)\n",
        "        and the gradient with respect to the parameter (_param_grad).\n",
        "\n",
        "        Args:\n",
        "            output_grad: The gradient of the loss with respect to the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            The gradient of the loss with respect to the input of this operation.\n",
        "        \"\"\"\n",
        "        assert_same_shape(self.output, output_grad)\n",
        "        # Calculates the gradient with respect to the input (_input_grad)\n",
        "        self.input_grad = self.input_grad(output_grad)\n",
        "        # Calculates the gradient with respect to the parameter (_param_grad).\n",
        "        self.param_grad = self._param_grad(output_grad)\n",
        "\n",
        "        assert_same_shape(self.input_, self.input_grad)\n",
        "        assert_same_shape(self.param, self.param_grad)\n",
        "\n",
        "        return self.input_grad\n",
        "\n",
        "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Calculates the gradient with respect to the parameter.\n",
        "        This method needs to be implemented by the subclasses.\n",
        "\n",
        "        Args:\n",
        "            output_grad: The gradient of the loss with respect to the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            The gradient of the loss with respect to the parameter.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError  # Your code here (but it's already provided)"
      ],
      "metadata": {
        "id": "pMoS1A7Om-BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Block 3: The WeightMultiply Class"
      ],
      "metadata": {
        "id": "KO084Y6HpjRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The WeightMultiply operation performs the crucial matrix multiplication between the input and the layer's weights. This is a core computation in every neural network.\n"
      ],
      "metadata": {
        "id": "hyH6jMGZp7BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Multiplication"
      ],
      "metadata": {
        "id": "xEjBnoJCqViB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input Data (Matrix X):\n",
        "\n",
        "# Sales in (Thousands of Dollars)\n",
        "X = np.array([\n",
        "    [10, 20, 15],  # Region 1: Product A, B, C\n",
        "    [25, 5, 12],   # Region 2\n",
        "    [8, 18, 22]    # Region 3\n",
        "])\n",
        "# Shape of X: (3 regions, 3 products)\n",
        "\n",
        "# Weight Matrix (Matrix W):\n",
        "  # This matrix represents the \"importance\" or \"contribution\"\n",
        "  # of each product to certain \"sales factors\"\n",
        "  # (which are abstract features the network learns).\n",
        "# For example, these factors could be things like\n",
        "  # \"market penetration,\" \"customer demand,\" or \"seasonal influence\"\n",
        "  # – things that aren't directly in our raw data.\n",
        "\n",
        "# Importance of each product to sales factors\n",
        "W = np.array([\n",
        "    [0.5, 1.2],   # Product A: Factor 1, Factor 2\n",
        "    [0.8, 0.3],   # Product B\n",
        "    [0.1, 1.0]    # Product C\n",
        "])\n",
        "# Shape of W: (3 products, 2 sales factors)\n",
        "\n",
        "# Resulting \"Sales Factor\" representation\n",
        "Result = np.dot(X, W)\n",
        "\n",
        "# Calculation for the first cell (Region 1, Factor 1):\n",
        "# (10 * 0.5) + (20 * 0.8) + (15 * 0.1) = 5 + 16 + 1.5 = 22.5\n",
        "\n",
        "[[22.5, 38.0],  # Region 1: Factor 1, Factor 2\n",
        " [17.5, 34.5],  # Region 2\n",
        " [26.4, 46.6]]  # Region 3\n",
        "# Shape of Result: (3 regions, 2 sales factors)"
      ],
      "metadata": {
        "id": "17gKwxX2qVAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightMultiply(ParamOperation):\n",
        "    \"\"\"\n",
        "    Weight multiplication operation for a neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, W: ndarray):\n",
        "        \"\"\"\n",
        "        Initialize Operation with self.param = W (weights).\n",
        "\n",
        "        Args:\n",
        "            W: The weight matrix.\n",
        "        \"\"\"\n",
        "        super().__init__(W)\n",
        "\n",
        "    def _output(self) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute output (y = xW).\n",
        "\n",
        "        Returns:\n",
        "            The result of the matrix multiplication.\n",
        "        \"\"\"\n",
        "        return np.dot(self.input_, self.param) # xW\n",
        "\n",
        "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute input gradient (dy/dx = W^T @ output_grad).\n",
        "\n",
        "        Args:\n",
        "            output_grad: The gradient of the loss with respect to the output.\n",
        "\n",
        "        Returns:\n",
        "            The gradient of the loss with respect to the input.\n",
        "        \"\"\"\n",
        "        return np.dot(output_grad, np.transpose(self.param, (1, 0)))\n",
        "\n",
        "\n",
        "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute parameter gradient (dy/dW = x^T @ output_grad).\n",
        "\n",
        "        Args:\n",
        "            output_grad: The gradient of the loss with respect to the output.\n",
        "\n",
        "        Returns:\n",
        "            The gradient of the loss with respect to the weights.\n",
        "        \"\"\"\n",
        "        return np.dot(np.transpose(self.input_, (1, 0)), output_grad)"
      ],
      "metadata": {
        "id": "mtv_h-iOp-Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiasAdd(ParamOperation):\n",
        "    \"\"\"\n",
        "    Compute bias addition.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, B: ndarray):\n",
        "        \"\"\"\n",
        "        Initialize Operation with self.param = B (bias).\n",
        "        Check appropriate shape.\n",
        "        \"\"\"\n",
        "        assert B.shape[0] == 1\n",
        "        super().__init__(B)\n",
        "\n",
        "    def _output(self) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute output (y = x + B).\n",
        "        \"\"\"\n",
        "        return self.input_ + self.param\n",
        "\n",
        "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute input gradient (dy/dx = 1 * output_grad = output_grad).\n",
        "        \"\"\"\n",
        "        return np.ones_like(self.input_) * output_grad # ∂L/∂x = ∂L/∂y\n",
        "\n",
        "    def _param_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute parameter gradient (dy/dB = sum(output_grad, axis=0)).\n",
        "        \"\"\"\n",
        "        param_grad = np.ones_like(self.param) * output_grad\n",
        "        return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1])  # ∂L/∂b"
      ],
      "metadata": {
        "id": "-5rIMGNeviLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid(Operation):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Pass\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "    def _output(self) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute output (sigmoid(x)).\n",
        "        \"\"\"\n",
        "        return 1.0 / (1.0 + np.exp(-1.0 * self.input_))\n",
        "\n",
        "    def _input_grad(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Compute input gradient (dy/dx = sigmoid(x) * (1 - sigmoid(x)) * output_grad).\n",
        "        \"\"\"\n",
        "        sigmoid_output = self.output  # Reuse the output from the forward pass\n",
        "        return sigmoid_output * (1.0 - sigmoid_output) * output_grad"
      ],
      "metadata": {
        "id": "mXqjThjKbSov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer"
      ],
      "metadata": {
        "id": "lgQsCZVrbgSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block 7: The Layer Class**\n",
        "\n",
        "A `Layer` in a neural network is a collection of neurons. It encapsulates the operations that transform data as it passes through that level of the network.\n",
        "\n",
        "1. Initialization (__init__):\n",
        "- Store the number of neurons in the layer.\n",
        "- Initialize lists to hold the layer's parameters (params), their gradients (param_grads), and the operations it performs (operations).\n",
        "- Keep track of whether it's the \"first\" time the layer is called (self.first). This is important for setting up the layer's parameters based on the input shape.\n",
        "\n",
        "2. Setup (_setup_layer):\n",
        "\n",
        "- This is an abstract method (like _output and _input_grad in Operation). Specific layer types (like Dense) will implement this to define their operations (e.g., weight multiplication, bias addition, activation).\n",
        "- It takes the input shape (num_in) as an argument, so it can initialize things like the weight matrix with the correct dimensions.\n",
        "\n",
        "3. Forward Pass (forward):\n",
        "\n",
        "- If it's the first time the layer is called, call _setup_layer to initialize the operations.\n",
        "- Store the input.\n",
        "- Iterate through the layer's operations, passing the input through each one in sequence. The output of each operation becomes the input to the next.\n",
        "- Store the final output of the layer.\n",
        "\n",
        "4. Backward Pass (backward):\n",
        "\n",
        "- Assert that the shape of the incoming output_grad matches the shape of the layer's output.\n",
        "- Iterate through the layer's operations in reverse order, passing the output_grad backward through each operation. The input_grad from each operation becomes the output_grad for the previous one.\n",
        "This is backpropagation!\n",
        "- Store the final input_grad (the gradient of the loss with respect to the input of this layer).\n",
        "- Call self._param_grads() to extract the parameter gradients from the operations.\n"
      ],
      "metadata": {
        "id": "aQt809Q-b1iI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Parameter Extraction (_params):\n",
        "\n",
        "Extract the layer's parameters from its operations.\n",
        "\n",
        "6. Parameter Gradient Extraction (_param_grads):\n",
        "\n",
        "Extract the layer's parameter gradients from its operations.\n"
      ],
      "metadata": {
        "id": "TuXB2OREcYrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer(object):\n",
        "    \"\"\"\n",
        "    A \"layer\" of neurons in a neural network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, neurons: int):\n",
        "        \"\"\"\n",
        "        The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer\n",
        "        \"\"\"\n",
        "        self.neurons = neurons\n",
        "        self.first = True  # Flag to indicate if it's the first forward pass\n",
        "        self.params: List[ndarray] = []  # List to store parameters (e.g., weights, biases)\n",
        "        self.param_grads: List[ndarray] = []  # List to store parameter gradients\n",
        "        self.operations: List[Operation] = []  # List to store operations in the layer\n",
        "\n",
        "    def _setup_layer(self, input_: ndarray) -> None:\n",
        "        \"\"\"\n",
        "        The _setup_layer function must be implemented for each layer\n",
        "        (e.g., Dense layer will set up WeightMultiply, BiasAdd, Activation)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()  # Abstract method\n",
        "\n",
        "    def forward(self, input_: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Passes input forward through a series of operations\n",
        "        \"\"\"\n",
        "        if self.first:\n",
        "          self._setup_layer(input_)\n",
        "          self.first = False\n",
        "\n",
        "        self.input_ = input_\n",
        "\n",
        "        for operation in self.operations:\n",
        "          input_ = operation.forward(input_)\n",
        "\n",
        "        self.output = input_\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_grad: ndarray) -> ndarray:\n",
        "        \"\"\"\n",
        "        Passes output_grad backward through a series of operations\n",
        "        Checks appropriate shapes\n",
        "        \"\"\"\n",
        "        assert_same_shape(self.output, output_grad)\n",
        "\n",
        "        for operation in reversed(self.operations):  # Iterate in reverse order\n",
        "            output_grad = operation.backward(output_grad)  # Backpropagate through each operation\n",
        "\n",
        "        input_grad = output_grad  # Store the final input gradient\n",
        "\n",
        "        self._param_grads()  # Extract parameter gradients\n",
        "\n",
        "        return input_grad\n",
        "\n",
        "\n",
        "    def _params(self) -> List[ndarray]:\n",
        "        \"\"\"\n",
        "        Extracts the _params from a layer's operations\n",
        "        \"\"\"\n",
        "        self.params = []\n",
        "        for operation in self.operations:\n",
        "            if isinstance(operation, ParamOperation):  # Check if it's a ParamOperation\n",
        "                self.params.append(operation.param)\n",
        "        return self.params\n",
        "\n",
        "\n",
        "    def _param_grads(self) -> List[ndarray]:\n",
        "        \"\"\"\n",
        "        Extracts the _param_grads from a layer's operations\n",
        "        \"\"\"\n",
        "        self.param_grads = []\n",
        "        for operation in self.operations:\n",
        "            if isinstance(operation, ParamOperation):  # Check if it's a ParamOperation\n",
        "                self.param_grads.append(operation.param_grad)\n",
        "        return self.param_grads"
      ],
      "metadata": {
        "id": "eFLP0gUGbvVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Dense(Layer):\n",
        "    \"\"\"\n",
        "    A fully connected layer which inherits from \"Layer\"\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 neurons: int,\n",
        "                 activation: Operation = Sigmoid()):\n",
        "        \"\"\"\n",
        "        Requires an activation function upon initialization\n",
        "        \"\"\"\n",
        "        super().__init__(neurons)  # Call the Layer's constructor\n",
        "        self.activation = activation\n",
        "\n",
        "    def _setup_layer(self, input_: ndarray) -> None:\n",
        "        \"\"\"\n",
        "        Defines the operations of a fully connected layer.\n",
        "        \"\"\"\n",
        "        # Initialize weights and biases\n",
        "        self.params = []\n",
        "        self.params.append(np.random.randn(input_.shape[1], self.neurons))  # Weights (W)\n",
        "        self.params.append(np.random.randn(1, self.neurons))  # Biases (B)\n",
        "\n",
        "        # Define the operations\n",
        "        self.operations = [\n",
        "            WeightMultiply(self.params[0]),  # input @ W\n",
        "            BiasAdd(self.params[1]),         # input @ W + B\n",
        "            self.activation                # activation(input @ W + B)\n",
        "        ]"
      ],
      "metadata": {
        "id": "_2WGJ74qddGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss"
      ],
      "metadata": {
        "id": "crPhj54Ld48x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss(object):\n",
        "  '''\n",
        "  The \"loss\" of a neural network.\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    '''Pass'''\n",
        "    pass\n",
        "\n",
        "  def forward(self, prediction: ndarray, target: ndarray) -> float:\n",
        "    '''\n",
        "    Computes the actual loss value.\n",
        "    '''\n",
        "    assert_same_shape(prediction, target)\n",
        "\n",
        "    self.prediction = prediction\n",
        "    self.target = target\n",
        "\n",
        "    loss_value = self._output()\n",
        "\n",
        "    return loss_value\n",
        "\n",
        "  def backward(self) -> ndarray:\n",
        "    '''\n",
        "    Computes gradient of the loss value with respect to the input to the\n",
        "    loss function.\n",
        "    '''\n",
        "    self.input_grad = self._input_grad()\n",
        "\n",
        "    assert_same_shape(self.prediction, self.input_grad)\n",
        "\n",
        "    return self.input_grad\n",
        "\n",
        "  def _output(self) -> float:\n",
        "    '''\n",
        "    Every subclass of \"Loss\" must implement the _output function.\n",
        "    '''\n",
        "    raise NotImplementedError()\n",
        "\n",
        "  def _input_grad(self) -> ndarray:\n",
        "    '''\n",
        "    Every subclass of \"Loss\" must implement the _input_grad function.\n",
        "    '''\n",
        "    raise NotImplementedError()"
      ],
      "metadata": {
        "id": "LNTLQ5JCd6BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanSquaredError(Loss):\n",
        "\n",
        "  def __init__(self):\n",
        "    '''Pass'''\n",
        "    super().__init__()\n",
        "\n",
        "  def _output(self) -> float:\n",
        "    '''\n",
        "    Computes the per-observation squared error loss.\n",
        "    '''\n",
        "    loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0]\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def _input_grad(self) -> ndarray:\n",
        "    '''\n",
        "    Computes the loss gradient with respect to the input for MSE loss.\n",
        "    '''\n",
        "\n",
        "    return 2.0 * (self.prediction - self.target) / self.prediction.shape[0]"
      ],
      "metadata": {
        "id": "V-1btXlpd8ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxCrossEntropyLoss(Loss):\n",
        "    def __init__(self, eps: float=1e-9)\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.single_output = False\n",
        "\n",
        "    def _output(self) -> float:\n",
        "\n",
        "        # applying the softmax function to each row (observation)\n",
        "        softmax_preds = softmax(self.prediction, axis=1)\n",
        "\n",
        "        # clipping the softmax output to prevent numeric instability\n",
        "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
        "\n",
        "        # actual loss computation\n",
        "        softmax_cross_entropy_loss = (\n",
        "            -1.0 * self.target * np.log(self.softmax_preds) - \\\n",
        "                (1.0 - self.target) * np.log(1 - self.softmax_preds)\n",
        "        )\n",
        "\n",
        "        return np.sum(softmax_cross_entropy_loss)\n",
        "\n",
        "    def _input_grad(self) -> ndarray:\n",
        "\n",
        "        return self.softmax_preds - self.target"
      ],
      "metadata": {
        "id": "krW26MrRd-Jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ],
      "metadata": {
        "id": "xyxMncG7eBl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(object):\n",
        "  '''\n",
        "  The class for a neural network.\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               layers: List[Layer], #List of Layer instances (e.g., [Dense(13, Sigmoid), Dense(1, Linear)]).\n",
        "               loss: Loss,\n",
        "               seed: float = 1):\n",
        "    '''\n",
        "    Neural networks need layers, and a loss.\n",
        "    '''\n",
        "    self.layers = layers # specifies the layer’s output dimension\n",
        "    self.loss = loss\n",
        "    self.seed = seed\n",
        "    if seed:\n",
        "      for layer in self.layers:\n",
        "        setattr(layer, 'seed', self.seed)\n",
        "\n",
        "  def forward(self, x_batch: ndarray) -> ndarray:\n",
        "    '''\n",
        "    Passes input forward through a series of layers.\n",
        "    '''\n",
        "    x_out = x_batch\n",
        "    for layer in self.layers:\n",
        "      x_out = layer.forward(x_out)\n",
        "\n",
        "    return x_out"
      ],
      "metadata": {
        "id": "ZHh5quzmeDaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Receive X and y as inputs, both ndarrays.\n",
        "# Feed X successively forward through each Layer.\n",
        "# Use the Loss to produce loss value and the loss gradient to be sent backward.\n",
        "# Use the loss gradient as input to the backward method for the network, which will calculate the param_grads for each layer in the network.\n",
        "# Call the update_params function on each layer, which will use the overall learning rate for the NeuralNetwork as well as the newly calculated param_grads.\n",
        "\n",
        "class NeuralNetwork(object):\n",
        "  '''\n",
        "  The class for a neural network.\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               layers: List[Layer], #List of Layer instances (e.g., [Dense(13, Sigmoid), Dense(1, Linear)]).\n",
        "               loss: Loss,\n",
        "               seed: float = 1):\n",
        "    '''\n",
        "    Neural networks need layers, and a loss.\n",
        "    '''\n",
        "    self.layers = layers # specifies the layer’s output dimension\n",
        "    self.loss = loss\n",
        "    self.seed = seed\n",
        "    if seed:\n",
        "      for layer in self.layers:\n",
        "        setattr(layer, 'seed', self.seed)\n",
        "\n",
        "  def forward(self, x_batch: ndarray) -> ndarray:\n",
        "    '''\n",
        "    Passes input forward through a series of layers.\n",
        "    '''\n",
        "    x_out = x_batch\n",
        "    for layer in self.layers:\n",
        "      x_out = layer.forward(x_out)\n",
        "\n",
        "    return x_out\n",
        "\n",
        "  def backward(self, loss_grad: ndarray) -> None:\n",
        "    '''\n",
        "    Passes data backward through a series of layers.\n",
        "    '''\n",
        "\n",
        "    grad = loss_grad\n",
        "    for layer in reversed(self.layers):\n",
        "      grad = layer.backward(grad)\n",
        "\n",
        "    return None\n",
        "\n",
        "  def train_batch(self,\n",
        "                  x_batch: ndarray,\n",
        "                  y_batch: ndarray) -> float:\n",
        "    '''\n",
        "    Passes data forward through the layers.\n",
        "    Computes the loss.\n",
        "    Passes data backward through the layers.\n",
        "    '''\n",
        "    predictions = self.forward(x_batch)\n",
        "    loss = self.loss.forward(predictions, y_batch)\n",
        "    self.backward(self.loss.backward())\n",
        "    return loss\n",
        "\n",
        "  #Provide iterators for parameters and their gradients across all layers.\n",
        "  def params(self):\n",
        "    '''\n",
        "    Gets the parameters for the network.\n",
        "    '''\n",
        "    for layer in self.layers:\n",
        "      yield from layer.params #Yields parameters (e.g., W1,b1,W2,b2,…W_1, b_1, W_2, b_2, \\ldotsW_1, b_1, W_2, b_2,).\n",
        "\n",
        "  def param_grads(self):\n",
        "    '''\n",
        "    Gets the gradient of the loss with respect to the parameters for the\n",
        "    network.\n",
        "    Yields gradients (e.g., ∂L∂W1,∂L∂b1,…)\n",
        "    '''\n",
        "    for layer in self.layers:\n",
        "      yield from layer.param_grads\n"
      ],
      "metadata": {
        "id": "JgiIRhw5SSEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and SGD"
      ],
      "metadata": {
        "id": "zz7zTOWSg-YD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Optimizer(object):\n",
        "  '''\n",
        "  Base class for a neural network optimizer.\n",
        "  '''\n",
        "  def __init__(self,\n",
        "               lr: float = 0.01):\n",
        "    '''\n",
        "    Every optimizer must have an initial learning rate.\n",
        "    '''\n",
        "    self.lr = lr\n",
        "\n",
        "  def step(self) -> None:\n",
        "    '''\n",
        "    Every optimizer must implement the \"step\" function.\n",
        "    '''\n",
        "    pass"
      ],
      "metadata": {
        "id": "J8X9gg91g_MU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD(Optimizer):\n",
        "  '''\n",
        "  Stochasitc gradient descent optimizer.\n",
        "  '''\n",
        "  def __init__(self,\n",
        "                lr: float = 0.01) -> None:\n",
        "    '''Pass'''\n",
        "    super().__init__(lr)\n",
        "\n",
        "  def step(self):\n",
        "    '''\n",
        "    For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment\n",
        "    based on the learning rate.\n",
        "    '''\n",
        "    for (param, param_grad) in zip(self.net.params(),\n",
        "                                    self.net.param_grads()):\n",
        "\n",
        "      param -= self.lr * param_grad"
      ],
      "metadata": {
        "id": "fIvEk4CEhDLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    '''\n",
        "    Trains a neural network\n",
        "    '''\n",
        "    def __init__(self,\n",
        "                 net: NeuralNetwork,\n",
        "                 optim: Optimizer) -> None:\n",
        "        '''\n",
        "        Requires a neural network and an optimizer in order for training to occur.\n",
        "        Assign the neural network as an instance variable to the optimizer.\n",
        "        '''\n",
        "        self.net = net\n",
        "        self.optim = optim\n",
        "        self.best_loss = 1e9\n",
        "        setattr(self.optim, 'net', self.net)\n",
        "    def generate_batches(self,\n",
        "                         X: ndarray,\n",
        "                         y: ndarray,\n",
        "                         size: int = 32) -> Tuple[ndarray]:\n",
        "        '''\n",
        "        Generates batches for training\n",
        "        '''\n",
        "        assert X.shape[0] == y.shape[0], \\\n",
        "        '''\n",
        "        features and target must have the same number of rows, instead\n",
        "        features has {0} and target has {1}\n",
        "        '''.format(X.shape[0], y.shape[0])\n",
        "\n",
        "        N = X.shape[0] # gets the total number of training examples (rows) in your dataset.\n",
        "\n",
        "        for ii in range(0, N, size): # 0, 32, 64, 96...\n",
        "\n",
        "            X_batch, y_batch = X[ii:ii+size], y[ii:ii+size]\n",
        "\n",
        "            yield X_batch, y_batch\n",
        "\n",
        "    def fit(self,\n",
        "            X_train: ndarray, y_train: ndarray,\n",
        "            X_test: ndarray, y_test: ndarray,\n",
        "            epochs: int=100,\n",
        "            eval_every: int=10,\n",
        "            batch_size: int=32,\n",
        "            seed: int = 1,\n",
        "            restart: bool = True)-> None:\n",
        "        '''\n",
        "        Fits the neural network on the training data for a certain number of epochs.\n",
        "        Every \"eval_every\" epochs, it evaluated the neural network on the testing data.\n",
        "        '''\n",
        "        np.random.seed(seed)\n",
        "        if restart:\n",
        "            for layer in self.net.layers:\n",
        "                layer.first = True\n",
        "\n",
        "            self.best_loss = 1e9\n",
        "\n",
        "        for e in range(epochs):\n",
        "            if (e+1) % eval_every == 0:\n",
        "                # for early stopping\n",
        "                last_model = deepcopy(self.net)\n",
        "            X_train, y_train = permute_data(X_train, y_train)\n",
        "            batch_generator = self.generate_batches(X_train, y_train,\n",
        "                                                    batch_size)\n",
        "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
        "              self.net.train_batch(X_batch, y_batch)\n",
        "              self.optim.step()\n",
        "\n",
        "            if (e+1) % eval_every == 0:\n",
        "\n",
        "                test_preds = self.net.forward(X_test)\n",
        "                loss = self.net.loss.forward(test_preds, y_test)\n",
        "\n",
        "                if loss < self.best_loss:\n",
        "                    print(f\"Validation loss after {e+1} epochs is {loss:.3f}\")\n",
        "                    self.best_loss = loss\n",
        "                else:\n",
        "                    print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\")\n",
        "                    self.net = last_model\n",
        "                    # ensure self.optim is still updating self.net\n",
        "                    setattr(self.optim, 'net', self.net)\n",
        "                    break\n",
        "\n"
      ],
      "metadata": {
        "id": "gwtoRDyshZt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}